{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________\n",
    "# __Machine Learning Project (using PySpark):__\n",
    "## __Accurately Classify \"Injury\" variable__\n",
    "#### By Varun Grewal  \n",
    "<a href=\"https://www.linkedin.com/in/varungrewal/\">LinkedIn</a> | <a href=\"https://github.com/varungrewal\">Github</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "### Relevant Links:        \n",
    "<a href=\"https://drive.google.com/file/d/1l2E1zqmXeG9cYv9l7QVbJC11romEIzjV/view?usp=sharing\">Data</a> | <a href=\"https://github.com/varungrewal/Machine-Learning-using-PySpark-/blob/main/Metadata.pdf\">Meta Data</a> \n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools Used: \n",
    "__Tool:__ Jupyter Notebook | __Language:__ Python       \n",
    "__Packages:__ NumPy | Pandas | PySpark\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='TOC'></a>\n",
    "### <u>Table of Contents:</u> \n",
    "1. [Introduction](#1)      \n",
    "    1.1 [Goal](#1.1)            \n",
    "2. [Data Description](#2)    \n",
    "    2.1 [Intialize](#2.1)     \n",
    "    2.2 [Understanding Raw Data](#2.2)      \n",
    "3. [ETL (Data Preparation, Cleaning, Wrangling, Manipulation and Check)](#3)                 \n",
    "4. [Machine Learning](#4)      \n",
    "    4.1 [Functions](#4.1)       \n",
    "    4.2 [PySpark Session and SQL](#4.2)       \n",
    "    4.3 [Model 1 - Logistic Regression](#4.3)           \n",
    "    4.4 [Model 2 - Random Forest](#4.4)            \n",
    "    4.5 [Model 3 - Gradient Boosted Trees](#4.5)       \n",
    "    4.6 [Model 4 - Linear Support Vector](#4.6)    \n",
    "5. [Conclusion](#5)\n",
    "\n",
    "<p style=\"color:green\"> Note: Select any cell and press TAB to come back to Table of Contents </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "<a id='1'></a>\n",
    "### 1. Introduction\n",
    "____\n",
    "\n",
    "This is the 3rd project in the series. Please see the links below to learn about the previous projects.   \n",
    "Project 1: <a href=\"https://github.com/varungrewal/Data-Analytics-Visualization\">Data Analytics and Visualization</a>              \n",
    "Project 2: <a href=\"https://github.com/varungrewal/Machine-Learning-using-Scikit-\">Machine Learning (using Scikit) </a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "<a id='1.1'></a>\n",
    "### 1.1 Goal \n",
    "_____\n",
    "The goal of this project is to build following four Classification models to accurately classify \"Injury\" variable \n",
    "   - Logistic Rgeression\n",
    "   - Random Forest\n",
    "   - Gradient Boosted Tree\n",
    "   - Linear Support Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "____ \n",
    "### 2. Data Description  \n",
    "____\n",
    "The raw data under consideration for this project is the 'Collision Data\" sourced from the Seattle Police Department for the year 2004-2020. Actual dataset is much larger. But, for this project I have limited the scope of the dataset to focus on one variable \"Injury Collision\".\n",
    "\n",
    "Preliminary analysis suggests that data is mostly clean and complete. However, some cleaning might be required to make it ideal for modeling and analysis. Size of the dataset is approx. 195K rows and 38 columns.\n",
    "Dataset is of medium complexity as there are multiple variables that can potentially impact the severity of the collision. Data is of mixed nature with integer, float, date and categorical variables being present. That means, it will require preprocessing and potentially normalization.  \n",
    "\n",
    "Note: Data is missing following important variables:\n",
    "- Age\n",
    "- Gender\n",
    "- Make/Model of the vehicle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.1'></a>\n",
    "_____\n",
    "### 2.1 Intialize:\n",
    "Import/Load all the required packages and the dataset\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark==2.4.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/5a/271c416c1c2185b6cb0151b29a91fff6fcaed80173c8584ff6d20e46b465/pyspark-2.4.5.tar.gz (217.8MB)\n",
      "\u001b[K     |████████████████████████████████| 217.8MB 85kB/s  eta 0:00:01   |█▎                              | 8.7MB 7.2MB/s eta 0:00:30     |█▉                              | 12.5MB 291kB/s eta 0:11:44/s eta 0:11:37                        | 20.8MB 291kB/s eta 0:11:15 |███▏                            | 21.3MB 291kB/s eta 0:11:14| 23.1MB 6.8MB/s eta 0:00:29     |██████▉                         | 46.4MB 5.9MB/s eta 0:00:30     |███████                         | 47.8MB 5.9MB/s eta 0:00:29     |███████▍                        | 50.0MB 5.9MB/s eta 0:00:29     |███████▌                        | 50.9MB 5.9MB/s eta 0:00:29     |███████▋                        | 51.5MB 5.9MB/s eta 0:00:29     |████████                        | 53.7MB 5.9MB/s eta 0:00:28     |████████                        | 54.5MB 5.9MB/s eta 0:00:28     |██████████▉                     | 73.8MB 6.9MB/s eta 0:00:21     |███████████                     | 75.2MB 6.9MB/s eta 0:00:21     |█████████████▏                  | 89.4MB 6.3MB/s eta 0:00:21    | 118.2MB 6.9MB/s eta 0:00:15     |███████████████████▊            | 134.3MB 5.5MB/s eta 0:00:16     |██████████████████████          | 150.2MB 5.5MB/s eta 0:00:139MB 7.0MB/s eta 0:00:09     |███████████████████████████     | 184.1MB 6.7MB/s eta 0:00:06     |███████████████████████████▏    | 184.8MB 6.7MB/s eta 0:00:05     |███████████████████████████▍    | 186.1MB 6.7MB/s eta 0:00:05     |███████████████████████████▌    | 187.5MB 6.7MB/s eta 0:00:05     |███████████████████████████▋    | 188.1MB 6.7MB/s eta 0:00:05     |████████████████████████████▍   | 193.0MB 6.2MB/s eta 0:00:05��█████████▌   | 194.3MB 6.2MB/s eta 0:00:04     |█████████████████████████████▎  | 199.5MB 6.2MB/s eta 0:00:03��█████████████████████  | 204.3MB 5.5MB/s eta 0:00:035.3MB 5.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting py4j==0.10.7 (from pyspark==2.4.5)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
      "\u001b[K     |████████████████████████████████| 204kB 36.4MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jupyterlab/.cache/pip/wheels/bf/db/04/61d66a5939364e756eb1c1be4ec5bdce6e04047fc7929a3c3c\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.7 pyspark-2.4.5\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark==2.4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark[sql] in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (2.4.5)\n",
      "Requirement already satisfied: py4j==0.10.7 in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from pyspark[sql]) (0.10.7)\n",
      "Requirement already satisfied: pandas>=0.19.2; extra == \"sql\" in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from pyspark[sql]) (1.1.5)\n",
      "Collecting pyarrow>=0.8.0; extra == \"sql\" (from pyspark[sql])\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/6b/f67ab1171433d380e60ee4f5dad0c8abe7f1246ff44cabbf45cdb3877dd5/pyarrow-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (17.9MB)\n",
      "\u001b[K     |████████████████████████████████| 17.9MB 5.4MB/s eta 0:00:01    |████████                        | 4.5MB 6.1MB/s eta 0:00:03     |████████████████████████▋       | 13.8MB 5.4MB/s eta 0:00:01��███████████████▉  | 16.7MB 5.4MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from pandas>=0.19.2; extra == \"sql\"->pyspark[sql]) (2020.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from pandas>=0.19.2; extra == \"sql\"->pyspark[sql]) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from pandas>=0.19.2; extra == \"sql\"->pyspark[sql]) (1.19.4)\n",
      "Requirement already satisfied: six>=1.5 in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas>=0.19.2; extra == \"sql\"->pyspark[sql]) (1.15.0)\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark[sql]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.ml.stat import Correlation, Summarizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier, LinearSVC\n",
    "from pyspark.ml.feature import PCA, VectorAssembler\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! pip install seaborn\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.style.use(['ggplot'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyterlab/conda/envs/python/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3072: DtypeWarning: Columns (33) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEVERITYCODE</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>OBJECTID</th>\n",
       "      <th>INCKEY</th>\n",
       "      <th>COLDETKEY</th>\n",
       "      <th>REPORTNO</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>ADDRTYPE</th>\n",
       "      <th>INTKEY</th>\n",
       "      <th>...</th>\n",
       "      <th>ROADCOND</th>\n",
       "      <th>LIGHTCOND</th>\n",
       "      <th>PEDROWNOTGRNT</th>\n",
       "      <th>SDOTCOLNUM</th>\n",
       "      <th>SPEEDING</th>\n",
       "      <th>ST_COLCODE</th>\n",
       "      <th>ST_COLDESC</th>\n",
       "      <th>SEGLANEKEY</th>\n",
       "      <th>CROSSWALKKEY</th>\n",
       "      <th>HITPARKEDCAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>-122.323148</td>\n",
       "      <td>47.70314</td>\n",
       "      <td>1</td>\n",
       "      <td>1307</td>\n",
       "      <td>1307</td>\n",
       "      <td>3502005</td>\n",
       "      <td>Matched</td>\n",
       "      <td>Intersection</td>\n",
       "      <td>37475.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Wet</td>\n",
       "      <td>Daylight</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>Entering at angle</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SEVERITYCODE           X         Y  OBJECTID  INCKEY  COLDETKEY REPORTNO  \\\n",
       "0             2 -122.323148  47.70314         1    1307       1307  3502005   \n",
       "\n",
       "    STATUS      ADDRTYPE   INTKEY  ... ROADCOND LIGHTCOND PEDROWNOTGRNT  \\\n",
       "0  Matched  Intersection  37475.0  ...      Wet  Daylight           NaN   \n",
       "\n",
       "   SDOTCOLNUM SPEEDING ST_COLCODE         ST_COLDESC  SEGLANEKEY  \\\n",
       "0         NaN      NaN         10  Entering at angle           0   \n",
       "\n",
       "   CROSSWALKKEY  HITPARKEDCAR  \n",
       "0             0             N  \n",
       "\n",
       "[1 rows x 38 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path='https://s3.us.cloud-object-storage.appdomain.cloud/cf-courses-data/CognitiveClass/DP0701EN/version-2/Data-Collisions.csv'\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.2'></a>\n",
    "_____\n",
    "### 2.2 Understanding Raw Data \n",
    "To get basic understanding (size,shape, etc.) of the dataset\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Data Dimensions (Rows/Columns): (194673, 38)\n"
     ]
    }
   ],
   "source": [
    "print('Raw Data Dimensions (Rows/Columns):',df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Names: \n",
      "-------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['SEVERITYCODE', 'X', 'Y', 'OBJECTID', 'INCKEY', 'COLDETKEY',\n",
       "       'REPORTNO', 'STATUS', 'ADDRTYPE', 'INTKEY', 'LOCATION',\n",
       "       'EXCEPTRSNCODE', 'EXCEPTRSNDESC', 'SEVERITYCODE.1', 'SEVERITYDESC',\n",
       "       'COLLISIONTYPE', 'PERSONCOUNT', 'PEDCOUNT', 'PEDCYLCOUNT',\n",
       "       'VEHCOUNT', 'INCDATE', 'INCDTTM', 'JUNCTIONTYPE', 'SDOT_COLCODE',\n",
       "       'SDOT_COLDESC', 'INATTENTIONIND', 'UNDERINFL', 'WEATHER',\n",
       "       'ROADCOND', 'LIGHTCOND', 'PEDROWNOTGRNT', 'SDOTCOLNUM', 'SPEEDING',\n",
       "       'ST_COLCODE', 'ST_COLDESC', 'SEGLANEKEY', 'CROSSWALKKEY',\n",
       "       'HITPARKEDCAR'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"Column Names: \")\n",
    "print(\"-------------------------------\")\n",
    "df.columns.values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     0,      1,      2, ..., 194670, 194671, 194672])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 194673 entries, 0 to 194672\n",
      "Data columns (total 38 columns):\n",
      " #   Column          Non-Null Count   Dtype  \n",
      "---  ------          --------------   -----  \n",
      " 0   SEVERITYCODE    194673 non-null  int64  \n",
      " 1   X               189339 non-null  float64\n",
      " 2   Y               189339 non-null  float64\n",
      " 3   OBJECTID        194673 non-null  int64  \n",
      " 4   INCKEY          194673 non-null  int64  \n",
      " 5   COLDETKEY       194673 non-null  int64  \n",
      " 6   REPORTNO        194673 non-null  object \n",
      " 7   STATUS          194673 non-null  object \n",
      " 8   ADDRTYPE        192747 non-null  object \n",
      " 9   INTKEY          65070 non-null   float64\n",
      " 10  LOCATION        191996 non-null  object \n",
      " 11  EXCEPTRSNCODE   84811 non-null   object \n",
      " 12  EXCEPTRSNDESC   5638 non-null    object \n",
      " 13  SEVERITYCODE.1  194673 non-null  int64  \n",
      " 14  SEVERITYDESC    194673 non-null  object \n",
      " 15  COLLISIONTYPE   189769 non-null  object \n",
      " 16  PERSONCOUNT     194673 non-null  int64  \n",
      " 17  PEDCOUNT        194673 non-null  int64  \n",
      " 18  PEDCYLCOUNT     194673 non-null  int64  \n",
      " 19  VEHCOUNT        194673 non-null  int64  \n",
      " 20  INCDATE         194673 non-null  object \n",
      " 21  INCDTTM         194673 non-null  object \n",
      " 22  JUNCTIONTYPE    188344 non-null  object \n",
      " 23  SDOT_COLCODE    194673 non-null  int64  \n",
      " 24  SDOT_COLDESC    194673 non-null  object \n",
      " 25  INATTENTIONIND  29805 non-null   object \n",
      " 26  UNDERINFL       189789 non-null  object \n",
      " 27  WEATHER         189592 non-null  object \n",
      " 28  ROADCOND        189661 non-null  object \n",
      " 29  LIGHTCOND       189503 non-null  object \n",
      " 30  PEDROWNOTGRNT   4667 non-null    object \n",
      " 31  SDOTCOLNUM      114936 non-null  float64\n",
      " 32  SPEEDING        9333 non-null    object \n",
      " 33  ST_COLCODE      194655 non-null  object \n",
      " 34  ST_COLDESC      189769 non-null  object \n",
      " 35  SEGLANEKEY      194673 non-null  int64  \n",
      " 36  CROSSWALKKEY    194673 non-null  int64  \n",
      " 37  HITPARKEDCAR    194673 non-null  object \n",
      "dtypes: float64(4), int64(12), object(22)\n",
      "memory usage: 56.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "_____\n",
    "### 3. ETL (Data Preparation, Cleaning, Wrangling, Manipulation and Check)\n",
    "____\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Dimensions (Rows/Columns) after cleaning: (178831, 41)\n",
      "----There is no missing data----\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>LOCATION</th>\n",
       "      <th>SEVERITYDESC</th>\n",
       "      <th>PERSONCOUNT</th>\n",
       "      <th>PEDCOUNT</th>\n",
       "      <th>PEDCYLCOUNT</th>\n",
       "      <th>VEHCOUNT</th>\n",
       "      <th>INATTENTIONIND</th>\n",
       "      <th>UNDERINFL</th>\n",
       "      <th>...</th>\n",
       "      <th>WEATHER-UNKNOWN</th>\n",
       "      <th>AT INTERSECTION (BUT NOT RELATED TO INTERSECTION)</th>\n",
       "      <th>AT INTERSECTION (INTERSECTION RELATED)</th>\n",
       "      <th>DRIVEWAY JUNCTION</th>\n",
       "      <th>MID-BLOCK (BUT INTERSECTION RELATED)</th>\n",
       "      <th>MID-BLOCK (NOT RELATED TO INTERSECTION)</th>\n",
       "      <th>RAMP JUNCTION</th>\n",
       "      <th>JUNCTIONTYPE-UNKNOWN</th>\n",
       "      <th>BLOCK</th>\n",
       "      <th>INTERSECTION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.323148</td>\n",
       "      <td>47.70314</td>\n",
       "      <td>5TH AVE NE AND NE 103RD ST</td>\n",
       "      <td>Injury</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            X         Y                    LOCATION SEVERITYDESC  PERSONCOUNT  \\\n",
       "0 -122.323148  47.70314  5TH AVE NE AND NE 103RD ST       Injury            2   \n",
       "\n",
       "   PEDCOUNT  PEDCYLCOUNT  VEHCOUNT  INATTENTIONIND  UNDERINFL  ...  \\\n",
       "0         0            0         2               0          0  ...   \n",
       "\n",
       "   WEATHER-UNKNOWN  AT INTERSECTION (BUT NOT RELATED TO INTERSECTION)  \\\n",
       "0                0                                                  0   \n",
       "\n",
       "   AT INTERSECTION (INTERSECTION RELATED) DRIVEWAY JUNCTION  \\\n",
       "0                                       1                 0   \n",
       "\n",
       "  MID-BLOCK (BUT INTERSECTION RELATED)  \\\n",
       "0                                    0   \n",
       "\n",
       "  MID-BLOCK (NOT RELATED TO INTERSECTION)  RAMP JUNCTION  \\\n",
       "0                                       0              0   \n",
       "\n",
       "   JUNCTIONTYPE-UNKNOWN  BLOCK INTERSECTION  \n",
       "0                     0      0            1  \n",
       "\n",
       "[1 rows x 80 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To be consistent, making all column labels as type string\n",
    "df.columns = list(map(str, df.columns))\n",
    "\n",
    "# Fixing datatype for DATETIME variables\n",
    "df[[\"INCDATE\"]] = df[[\"INCDATE\"]].astype(\"datetime64\")\n",
    "df[[\"INCDTTM\"]] = df[[\"INCDTTM\"]].astype(\"datetime64\")\n",
    "\n",
    "# Renaming the Severity variables to improve readability\n",
    "df[\"SEVERITYDESC\"].replace(\"Property Damage Only Collision\", \"Property Damage\", inplace=True)\n",
    "df[\"SEVERITYDESC\"].replace(\"Injury Collision\", \"Injury\", inplace=True)\n",
    "\n",
    "# Adding needed columns for analysis of Big Picture\n",
    "df[[\"COMB\"]] = df['SEVERITYDESC']+\"/\"+df['COLLISIONTYPE']+\"/\"+df['JUNCTIONTYPE']\n",
    "df[\"COMB\"] = df.COMB.astype(str)\n",
    "df[[\"COMB-COND\"]] = df['WEATHER']+\"/\"+df['ROADCOND']+\"/\"+df['LIGHTCOND']\n",
    "df[\"COMB-COND\"] = df[\"COMB-COND\"].astype(str)\n",
    "\n",
    "# Adding needed columns for analysis of DATETIME variables\n",
    "df['DATE'] = pd.to_datetime(df['INCDTTM'], format='%d-%m-%y', errors='coerce').dt.floor('D')\n",
    "df['YEAR'] = pd.DatetimeIndex(df['INCDTTM']).year\n",
    "df['MONTH'] = pd.DatetimeIndex(df['INCDTTM']).month\n",
    "df['DAY'] = pd.DatetimeIndex(df['INCDTTM']).day\n",
    "df['WEEKDAY'] = df['DATE'].dt.day_name()\n",
    "df['WEEKDAYNUM'] = df['DATE'].dt.dayofweek\n",
    "df['TIME'] = pd.DatetimeIndex(df['INCDTTM']).time\n",
    "df['TIME2']=pd.to_datetime(df['INCDTTM']).dt.strftime('%I:%M %p')\n",
    "df['TIME3']=pd.to_datetime(df['INCDTTM']).dt.strftime('%p')\n",
    "\n",
    "# Adding needed columns for Business/Finance inspired metrics\n",
    "bins = [1,3,6,9,12]\n",
    "quarter = [\"Q1\",\"Q2\",\"Q3\",\"Q4\"]\n",
    "df['QUARTER'] = pd.cut(df['MONTH'], bins, labels=quarter,include_lowest=True)\n",
    "df['QUARTER'] = df.QUARTER.astype(str)\n",
    "df['YR-QTR'] = df['YEAR'].astype(\"str\")+ \"-\" + df['QUARTER']\n",
    "\n",
    "# Adding needed columns for Seasonal effect metrics\n",
    "bins2 = [1,2,5,8,11,12]\n",
    "season = [\"WINTER\",\"SPRING\",\"SUMMER\",\"FALL\",\"WINTER\"]\n",
    "df['SEASON'] = pd.cut(df['MONTH'], bins2, labels=season,ordered=False, include_lowest=True)\n",
    "bins3 = [0,1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "rainfall = [5.2,3.9,3.3,2.0,1.6,1.4,0.6,0.8,1.7,3.3,5.0,5.4]\n",
    "df['AVGRAINFALL-INCHES'] = pd.cut(df['MONTH'], bins3, labels=rainfall,ordered=False, include_lowest=True)\n",
    "temp = [45,48,52,56,64,69,72,73,67,59,51,47]\n",
    "df['AVGTEMP-F'] = pd.cut(df['MONTH'], bins3, labels=temp,ordered=False, include_lowest=True)\n",
    "daylight = [9,10,12,14,15,16,16,14,13,11,9,9]\n",
    "df['AVGDAYLIGHT-HRS'] = pd.cut(df['MONTH'], bins3, labels=daylight,ordered=False, include_lowest=True)\n",
    "df[['AVGRAINFALL-INCHES']] = df[['AVGRAINFALL-INCHES']].astype(\"float\")\n",
    "df[['AVGTEMP-F']] = df[[\"AVGTEMP-F\"]].astype(\"int\")\n",
    "df[['AVGDAYLIGHT-HRS']] = df[[\"AVGDAYLIGHT-HRS\"]].astype(\"int\")\n",
    "\n",
    "# Adding needed columns for analysis of GPS variable\n",
    "df[\"GPS\"] = round(df['X'],7).astype(\"str\")+ \",\"+round(df['Y'],7).astype(\"str\")\n",
    "\n",
    "# Dropping unnecessary columns\n",
    "df.drop(['OBJECTID','INCKEY','INTKEY','COLDETKEY','REPORTNO','STATUS','SEVERITYCODE.1','INCDTTM','INCDATE','EXCEPTRSNCODE','EXCEPTRSNDESC', 'SDOTCOLNUM', 'SEGLANEKEY', 'CROSSWALKKEY', 'ST_COLCODE'], axis=1, inplace=True)\n",
    "df.head(1)\n",
    "\n",
    "# list of columns after changes\n",
    "df.columns\n",
    "\n",
    "# To see if dataset has any missing rows\n",
    "missing_data = df.isnull()\n",
    "missing_data.head(1)\n",
    "\n",
    "# To identiy and list columns with missing values\n",
    "#for column in missing_data.columns.values.tolist():\n",
    "  #  print(column)\n",
    "   # print (missing_data[column].value_counts())\n",
    "   # print(\"________________________________________\") \n",
    "\n",
    "# Dropping missing data rows to make sure data is complete\n",
    "df.dropna(subset=[\"X\"], axis=0, inplace=True)\n",
    "df.dropna(subset=[\"COLLISIONTYPE\"], axis=0, inplace=True)\n",
    "df.dropna(subset=[\"UNDERINFL\"], axis=0, inplace=True)\n",
    "df.dropna(subset=[\"ROADCOND\"], axis=0, inplace=True)\n",
    "df.dropna(subset=[\"JUNCTIONTYPE\"], axis=0, inplace=True)\n",
    "df.dropna(subset=[\"WEATHER\"], axis=0, inplace=True)\n",
    "df.dropna(subset=[\"LIGHTCOND\"], axis=0, inplace=True)\n",
    "\n",
    "# Drop incomplete data i.e. Year 2020\n",
    "df.drop(df[df.YEAR > 2019].index, inplace=True)\n",
    "\n",
    "# Reset index, because we dropped rows\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print('Data Dimensions (Rows/Columns) after cleaning:',df.shape)\n",
    "df.head(1)\n",
    "\n",
    "# Steps to prepare data for future analysis\n",
    "\n",
    "# Converting Y/N to 1/0\n",
    "df[\"UNDERINFL\"].replace(\"N\", 0, inplace=True)\n",
    "df[\"UNDERINFL\"].replace(\"Y\", 1, inplace=True)\n",
    "df[\"HITPARKEDCAR\"].replace(\"N\", 0, inplace=True)\n",
    "df[\"HITPARKEDCAR\"].replace(\"Y\", 1, inplace=True)\n",
    "# Filling missing values\n",
    "df[\"PEDROWNOTGRNT\"].replace(np.nan, 0, inplace=True)\n",
    "df[\"PEDROWNOTGRNT\"].replace(\"Y\", 1, inplace=True)\n",
    "df[\"SPEEDING\"].replace(np.nan, 0, inplace=True)\n",
    "df[\"SPEEDING\"].replace(\"Y\", 1, inplace=True)\n",
    "df[\"INATTENTIONIND\"].replace(np.nan, 0, inplace=True)\n",
    "df[\"INATTENTIONIND\"].replace(\"Y\", 1, inplace=True)\n",
    "\n",
    "# Correcting datatype\n",
    "df[[\"UNDERINFL\"]] = df[[\"UNDERINFL\"]].astype(\"int\")\n",
    "df[[\"PEDROWNOTGRNT\"]] = df[[\"PEDROWNOTGRNT\"]].astype(\"int\")\n",
    "df[[\"SPEEDING\"]] = df[[\"SPEEDING\"]].astype(\"int\")\n",
    "df[[\"INATTENTIONIND\"]] = df[[\"INATTENTIONIND\"]].astype(\"int\")\n",
    "df[[\"HITPARKEDCAR\"]] = df[[\"HITPARKEDCAR\"]].astype(\"int\")\n",
    "df[['YEAR']] = df[['YEAR']].astype(\"int\")\n",
    "df[['MONTH']] = df[['MONTH']].astype(\"int\")\n",
    "df[['DAY']] = df[['DAY']].astype(\"int\")\n",
    "\n",
    "# adding columns for analysis of state of mind\n",
    "df[[\"COMB-MIND\"]] = df['INATTENTIONIND']+df['UNDERINFL']+df['SPEEDING']\n",
    "df[\"COMB-MIND\"] = df[\"COMB-MIND\"].astype(int)\n",
    "df.head(1)\n",
    "\n",
    "\n",
    "# Check missing data\n",
    "missing_data = df.isnull()\n",
    "#for column in missing_data.columns.values.tolist():\n",
    "  #  print(column)\n",
    "   # print (missing_data[column].value_counts())\n",
    "   # print(\"________________________________________\") \n",
    "if missing_data.bool == True:\n",
    "    print(\"----There is still missing data----\")\n",
    "else:\n",
    "    print(\"----There is no missing data----\")\n",
    "\n",
    "# Print unique values and its count for each column\n",
    "col_name = df.columns.tolist()\n",
    "row_num = df.index.tolist()\n",
    "\n",
    "#for i,x in enumerate(col_name):\n",
    "   # print (\"Unique value count of: \", x)\n",
    "    #print (\"------------------------------------------\")\n",
    "    #print(df[x].value_counts())\n",
    "   # print (\"__________________________________________\")\n",
    "\n",
    "\n",
    "# create dummy variable to split SEVERITYDESC\n",
    "dummy_var = pd.get_dummies(df[\"SEVERITYDESC\"])\n",
    "dum_list = dummy_var.columns.values.tolist()\n",
    "dum_list2 = [x.upper() for x in dum_list]\n",
    "#print(dum_list2)\n",
    "dummy_var.columns = dum_list2\n",
    "#dummy_var.head(1)\n",
    "\n",
    "# create dummy variable to split COLLISIONTYPE\n",
    "dummy_var1 = pd.get_dummies(df[\"COLLISIONTYPE\"])\n",
    "dum_list = dummy_var1.columns.values.tolist()\n",
    "#dummy_var1.head(1)\n",
    "dum_list2 = [x.upper() for x in dum_list]\n",
    "#print(dum_list2)\n",
    "dummy_var1.columns = dum_list2\n",
    "dummy_var1.rename(columns={'OTHER':'COLLISIONTYPE-OTHER'}, inplace=True)\n",
    "#dummy_var1.head(1)\n",
    "\n",
    "# create dummy variable to split ROADCOND\n",
    "dummy_var2 = pd.get_dummies(df[\"ROADCOND\"])\n",
    "dum_list = dummy_var2.columns.values.tolist()\n",
    "#dummy_var2.head(1)\n",
    "dum_list2 = [x.upper() for x in dum_list]\n",
    "#print(dum_list2)\n",
    "dummy_var2.columns = dum_list2\n",
    "dummy_var2.rename(columns={'OTHER':'ROADCOND-OTHER'}, inplace=True)\n",
    "dummy_var2.rename(columns={'UNKNOWN':'ROADCOND-UNKNOWN'}, inplace=True)\n",
    "#dummy_var2.head(1)\n",
    "\n",
    "# create dummy variable to split LIGHTCOND\n",
    "dummy_var3 = pd.get_dummies(df[\"LIGHTCOND\"])\n",
    "dum_list = dummy_var3.columns.values.tolist()\n",
    "#dummy_var3.head(1)\n",
    "dum_list2 = [x.upper() for x in dum_list]\n",
    "#print(dum_list2)\n",
    "dummy_var3.columns = dum_list2\n",
    "dummy_var3.rename(columns={'OTHER':'LIGHTCOND-OTHER'}, inplace=True)\n",
    "dummy_var3.rename(columns={'UNKNOWN':'LIGHTCOND-UNKNOWN'}, inplace=True)\n",
    "#dummy_var3.head(1)\n",
    "\n",
    "# create dummy variable to split WEATHER\n",
    "dummy_var4 = pd.get_dummies(df[\"WEATHER\"])\n",
    "dum_list = dummy_var4.columns.values.tolist()\n",
    "#dummy_var3.head(1)\n",
    "dum_list2 = [x.upper() for x in dum_list]\n",
    "#print(dum_list2)\n",
    "dummy_var4.columns = dum_list2\n",
    "dummy_var4.rename(columns={'OTHER':'WEATHER-OTHER'}, inplace=True)\n",
    "dummy_var4.rename(columns={'UNKNOWN':'WEATHER-UNKNOWN'}, inplace=True)\n",
    "#dummy_var4.head(1)\n",
    "\n",
    "# create dummy variable to split JUNCTIONTYPE\n",
    "dummy_var5 = pd.get_dummies(df[\"JUNCTIONTYPE\"])\n",
    "dum_list = dummy_var5.columns.values.tolist()\n",
    "#dummy_var3.head(1)\n",
    "dum_list2 = [x.upper() for x in dum_list]\n",
    "#print(dum_list2)\n",
    "dummy_var5.columns = dum_list2\n",
    "dummy_var5.rename(columns={'UNKNOWN':'JUNCTIONTYPE-UNKNOWN'}, inplace=True)\n",
    "#dummy_var5.head(1)\n",
    "\n",
    "## create dummy variable to split ADDRTYPE\n",
    "dummy_var6 = pd.get_dummies(df[\"ADDRTYPE\"])\n",
    "dum_list = dummy_var6.columns.values.tolist()\n",
    "#dummy_var3.head(1)\n",
    "dum_list2 = [x.upper() for x in dum_list]\n",
    "#print(dum_list2)\n",
    "dummy_var6.columns = dum_list2\n",
    "#dummy_var6.head(1)\n",
    "\n",
    "# merge dummy variables with df_ds (dataframe intialized for Data Science model)\n",
    "df_ds = pd.concat([df, dummy_var,dummy_var1,dummy_var2,\n",
    "                   dummy_var3,dummy_var4,dummy_var5,dummy_var6], axis=1)\n",
    "\n",
    "# Dropping unnecessary columns\n",
    "df_ds.drop(['SEVERITYCODE', 'ADDRTYPE','COLLISIONTYPE', \n",
    "            'JUNCTIONTYPE', 'SDOT_COLDESC','SDOT_COLCODE',\n",
    "            'WEATHER', 'ROADCOND', 'LIGHTCOND','ST_COLDESC'],  \n",
    "             axis=1, inplace=True)\n",
    "df_ds.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "_____\n",
    "### 4. Machine Learning \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4.1'></a>\n",
    "____\n",
    "#### 4.1 Functions \n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    " def spk_classification(classificationmodel,df_spk_train,df_spk_test): \n",
    "    #prediction_train = None\n",
    "    #prediction_test = None\n",
    "    global pipeline\n",
    "    pipeline = Pipeline(stages=[vectorAssembler,classificationmodel])\n",
    "    model_train = pipeline.fit(df_spk_train)\n",
    "    prediction_train = model_train.transform(df_spk_train)\n",
    "    #model_test = pipeline.fit(df_spk_test)\n",
    "    #prediction_test = model_test.transform(df_spk_test)\n",
    "    prediction_test = model_train.transform(df_spk_test)\n",
    "    spk_classificationevaluation(prediction_train,prediction_test,model_train)\n",
    "   \n",
    "def spk_classificationevaluation(prediction_train,prediction_test,model_train):\n",
    "    if classificationmodel == lr:\n",
    "        print(\"Model: Logistic Regression\",\"\\n--------------------------------------\" )\n",
    "    if classificationmodel == rf:\n",
    "        print(\"Model: Random Forest\",\"\\n--------------------------------------\" )\n",
    "    if classificationmodel == gbt:\n",
    "        print(\"Model: Gradient Boosted Tree\",\"\\n--------------------------------------\" )\n",
    "    if classificationmodel == lsvc:\n",
    "        print(\"Model: Linear Support Vector Machine\",\"\\n--------------------------------------\" )   \n",
    "    global evaluation\n",
    "    evaluation = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"INJURY\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    print(\"Description:\", model_train.stages[1])\n",
    "    accuracy_train = evaluation.evaluate(prediction_train)\n",
    "    print_spk_testtrainsetinfo(df_spk_train,df_spk_test)\n",
    "    print(\"Trainset Accuracy:\", round(accuracy_train,3))\n",
    "    print(\"Trainset Error = %g\" % round((1.0 - accuracy_train),3))\n",
    "    accuracy_test = evaluation.evaluate(prediction_test)\n",
    "    print(\"Testset Accuracy:\",round(evaluation.evaluate(prediction_test),3))\n",
    "    print(\"Test Error = %g\" % round((1.0 - accuracy_test),3))\n",
    "    spk_checkmodeloutput(prediction_train,prediction_test)\n",
    "\n",
    "def spk_checkmodeloutput(prediction_train,prediction_test):\n",
    "    #prediction_test.createOrReplaceTempView(\"prediction_test\")\n",
    "    #print(\"\\nCheck Results: \\n--------------------------------------\" )\n",
    "    #prediction_test.select(\"Injury\",\"rawPrediction\",\"probability\",\"prediction\").show(2)\n",
    "    #spark.sql(\"\"\"SELECT Injury, rawPrediction, probability, prediction \n",
    "               #from prediction_test where Injury=0 AND prediction =1\"\"\").show(2)\n",
    "    print(\"\\nTrainset Results: \" )\n",
    "    prediction_train.groupBy(\"Injury\",\"prediction\").count().show()\n",
    "    print(\"\\nTestset Results: \" )\n",
    "    prediction_test.groupBy(\"Injury\",\"prediction\").count().show()\n",
    "\n",
    "def print_spk_testtrainsetinfo(df_spk_train,df_spk_test):\n",
    "    print(\"Trainset Size (%):\",round(spk_trainsize,2),\"\\nTestset Size (%):\", round(spk_testsize,2))\n",
    "    print(\"Trainset Count:\",df_spk_train.count(),\"\\nTestset Count:\", df_spk_test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spk_crossvalidation(classificationmodel,pipeline,evaluation,df_spk_train,df_spk_test):   \n",
    "    if classificationmodel == rf: \n",
    "        paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(classificationmodel.numTrees, [3, 10]) \\\n",
    "        .build()\n",
    "    if classificationmodel == lr:\n",
    "        paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(classificationmodel.regParam, [0.1, 0.01]) \\\n",
    "        .addGrid(classificationmodel.elasticNetParam, [0, 1]).build()\n",
    "    if classificationmodel == lsvc:\n",
    "        paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(classificationmodel.regParam, [0.1, 0.01]) \\\n",
    "        .addGrid(classificationmodel.maxIter, [3, 10]).build()\n",
    "    if classificationmodel == gbt: \n",
    "        paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(classificationmodel.maxIter, [3, 10]) \\\n",
    "        .build()\n",
    "\n",
    "    crossval = CrossValidator(estimator=pipeline, \\\n",
    "                              estimatorParamMaps=paramGrid,\\\n",
    "                              evaluator=evaluation, \\\n",
    "                              numFolds=4)  # use 3+ folds in practice\n",
    "    cv_model = crossval.fit(df_spk_train)\n",
    "    cv_prediction = cv_model.transform(df_spk_test)\n",
    "    cv_selected = cv_prediction.select(\"INJURY\",\"prediction\")\n",
    "    print(\"Cross Validation Results:\\n--------------------------------------\")\n",
    "    cv_selected.groupBy(\"Injury\",\"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4.2'></a>\n",
    "____\n",
    "#### 4.2 PySpark Session and SQL\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark= df_ds[['INATTENTIONIND', 'UNDERINFL',\n",
    "       'PEDROWNOTGRNT', 'SPEEDING', 'HITPARKEDCAR', \n",
    "        'AVGRAINFALL-INCHES','AVGTEMP-F', 'AVGDAYLIGHT-HRS', 'INJURY',\n",
    "       'PROPERTY DAMAGE', 'ANGLES', 'CYCLES', 'HEAD ON', 'LEFT TURN',\n",
    "       'COLLISIONTYPE-OTHER', 'PARKED CAR', 'PEDESTRIAN', 'REAR ENDED',\n",
    "       'RIGHT TURN', 'SIDESWIPE', 'DRY', 'ICE', 'OIL', 'ROADCOND-OTHER',\n",
    "       'SAND/MUD/DIRT', 'SNOW/SLUSH', 'STANDING WATER', 'ROADCOND-UNKNOWN',\n",
    "       'WET', 'DARK - NO STREET LIGHTS', 'DARK - STREET LIGHTS OFF',\n",
    "       'DARK - STREET LIGHTS ON', 'DAWN', 'DAYLIGHT', 'DUSK',\n",
    "       'LIGHTCOND-OTHER', 'LIGHTCOND-UNKNOWN', 'BLOWING SAND/DIRT', 'CLEAR',\n",
    "       'FOG/SMOG/SMOKE', 'WEATHER-OTHER', 'OVERCAST', 'RAINING',\n",
    "       'SEVERE CROSSWIND', 'SLEET/HAIL/FREEZING RAIN', 'SNOWING',\n",
    "       'WEATHER-UNKNOWN', 'AT INTERSECTION (BUT NOT RELATED TO INTERSECTION)',\n",
    "       'AT INTERSECTION (INTERSECTION RELATED)', 'DRIVEWAY JUNCTION',\n",
    "       'MID-BLOCK (BUT INTERSECTION RELATED)',\n",
    "       'MID-BLOCK (NOT RELATED TO INTERSECTION)', 'RAMP JUNCTION',\n",
    "       'JUNCTIONTYPE-UNKNOWN', 'BLOCK', 'INTERSECTION']]\n",
    "\n",
    "df_spark.columns = ['INATTENTIONIND', 'UNDERINFL',\n",
    "       'PEDROWNOTGRNT', 'SPEEDING', 'HITPARKEDCAR', \n",
    "        'AVGRAINFALL_INCHES','AVGTEMP_F', 'AVGDAYLIGHT_HRS', 'INJURY',\n",
    "       'PROPERTY_DAMAGE', 'ANGLES', 'CYCLES', 'HEAD_ON', 'LEFT_TURN',\n",
    "       'COLLISIONTYPE_OTHER', 'PARKED_CAR', 'PEDESTRIAN', 'REAR_ENDED',\n",
    "       'RIGHT_TURN', 'SIDESWIPE', 'DRY', 'ICE', 'OIL', 'ROADCOND_OTHER',\n",
    "       'SAND_MUD_DIRT', 'SNOW_SLUSH', 'STANDING_WATER', 'ROADCOND_UNKNOWN',\n",
    "       'WET', 'DARK_NO_STREET_LIGHTS', 'DARK_STREET_LIGHTS_OFF',\n",
    "       'DARK_STREET_LIGHTS_ON', 'DAWN', 'DAYLIGHT', 'DUSK',\n",
    "       'LIGHTCOND_OTHER', 'LIGHTCOND_UNKNOWN', 'BLOWING_SAND_DIRT', 'CLEAR',\n",
    "       'FOG_SMOG_SMOKE', 'WEATHER_OTHER', 'OVERCAST', 'RAINING',\n",
    "       'SEVERE_CROSSWIND', 'SLEET_HAIL_FREEZING_RAIN', 'SNOWING',\n",
    "       'WEATHER_UNKNOWN', 'AT_INTERSECTION_BUT_NOT_RELATED_TO_INTERSECTION',\n",
    "       'AT_INTERSECTION_INTERSECTION_RELATED', 'DRIVEWAY_JUNCTION',\n",
    "       'MID_BLOCK_BUT_INTERSECTION_RELATED',\n",
    "       'MID_BLOCK_NOT RELATED_TO_INTERSECTION', 'RAMP_JUNCTION',\n",
    "       'JUNCTIONTYPE_UNKNOWN', 'BLOCK', 'INTERSECTION']\n",
    "\n",
    "df_spark.columns = list(map(str, df_spark.columns))\n",
    "\n",
    "df_spk_use = spark.createDataFrame(df_spark)\n",
    "\n",
    "df_spk_use.createOrReplaceTempView(\"df_spk_use\")\n",
    "#spark.sql(\"SELECT * from df_spk_use\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_spk_use.printSchema()\n",
    "#df_spk_use.select(\"Injury\",\"Property_Damage\").show(3)\n",
    "#df_spk_use.filter((df_spk_use[\"Injury\"] > 0) & (df_spk_use[\"UNDERINFL\"] > 0)).show(1)\n",
    "#df_spk_use.groupBy(\"Injury\",\"Property_Damage\",\"Clear\", \"ANGLES\").count().show()\n",
    "#spark.sql(\"\"\"SELECT SEVERITYDESC, INJURY,PROPERTY_DAMAGE, ANGLES, \n",
    "         # CYCLES, HEAD_ON, LEFT_TURN from df_spk_use\"\"\").show(2)\n",
    "#spark.sql(\"SELECT INJURY as count from df_spk_use\").count()\n",
    "#spark.sql(\"SELECT INJURY as cnt from df_spk_use\").first().cnt\n",
    "'''spark.sql(\"\"\"SELECT count(Injury) as Count, \n",
    "          max(Injury) as Max, \n",
    "          min(Injury) as Min, \n",
    "          round(mean(Injury),3) as Mean,\n",
    "          round(stddev_pop(Injury),3) as Std,\n",
    "          round(skewness(Injury),3) as Skewness,\n",
    "          round(kurtosis(injury),3) as Kurtosis\n",
    "          from df_spk_use\"\"\").show()'''    \n",
    "#spark.sql(\"SELECT round(corr(Injury,Clear),3) as Correlation from df_spk_use\").show()\n",
    "#Injury = spark.sql(\"SELECT Injury, clear, dry, angles FROM df_spk_use WHERE clear > 0 AND dry < 1\")\n",
    "#Injury.show(3)\n",
    "#df_spk_use[\"Injury\", \"Clear\", \"Dry\"].describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorAssembler = VectorAssembler(inputCols= ['INATTENTIONIND', 'UNDERINFL',\n",
    "       'PEDROWNOTGRNT', 'SPEEDING', 'HITPARKEDCAR', \n",
    "        'AVGRAINFALL_INCHES','AVGTEMP_F', 'AVGDAYLIGHT_HRS','INJURY','ANGLES', \n",
    "        'CYCLES', 'HEAD_ON', 'LEFT_TURN',  \n",
    "        'PARKED_CAR', 'PEDESTRIAN', 'REAR_ENDED',\n",
    "       'RIGHT_TURN', 'SIDESWIPE', 'DRY', \n",
    "       'DARK_NO_STREET_LIGHTS', 'DARK_STREET_LIGHTS_OFF',\n",
    "       'DARK_STREET_LIGHTS_ON',  'DAYLIGHT', \n",
    "        'CLEAR','OVERCAST', 'RAINING',\n",
    "       'AT_INTERSECTION_BUT_NOT_RELATED_TO_INTERSECTION',\n",
    "       'AT_INTERSECTION_INTERSECTION_RELATED', \n",
    "       'MID_BLOCK_BUT_INTERSECTION_RELATED',\n",
    "       'MID_BLOCK_NOT RELATED_TO_INTERSECTION', 'RAMP_JUNCTION'], \n",
    "        outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_topredict = \"INJURY\"\n",
    "lr = LogisticRegression(maxIter = 10, regParam = 0.3, elasticNetParam = 0.8,labelCol=var_topredict)\n",
    "rf = RandomForestClassifier(labelCol= var_topredict, featuresCol=\"features\", numTrees=10)\n",
    "gbt = GBTClassifier(labelCol=var_topredict, featuresCol=\"features\", maxIter=10)\n",
    "lsvc = LinearSVC(maxIter=10, regParam=0.1,labelCol=var_topredict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4.3'></a>\n",
    "____\n",
    "#### 4.3 Model 1 - Logistic Regression\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic Regression \n",
      "--------------------------------------\n",
      "Description: LogisticRegressionModel: uid = LogisticRegression_26a0e72cd4ae, numClasses = 2, numFeatures = 31\n",
      "Trainset Size (%): 0.75 \n",
      "Testset Size (%): 0.25\n",
      "Trainset Count: 134155 \n",
      "Testset Count: 44676\n",
      "Trainset Accuracy: 1.0\n",
      "Trainset Error = 0\n",
      "Testset Accuracy: 1.0\n",
      "Test Error = 0\n",
      "\n",
      "Trainset Results: \n",
      "+------+----------+-----+\n",
      "|Injury|prediction|count|\n",
      "+------+----------+-----+\n",
      "|     0|       0.0|92671|\n",
      "|     1|       1.0|41484|\n",
      "+------+----------+-----+\n",
      "\n",
      "\n",
      "Testset Results: \n",
      "+------+----------+-----+\n",
      "|Injury|prediction|count|\n",
      "+------+----------+-----+\n",
      "|     0|       0.0|30746|\n",
      "|     1|       1.0|13930|\n",
      "+------+----------+-----+\n",
      "\n",
      "Cross Validation Results:\n",
      "--------------------------------------\n",
      "+------+----------+-----+\n",
      "|Injury|prediction|count|\n",
      "+------+----------+-----+\n",
      "|     0|       0.0|30746|\n",
      "|     1|       1.0|13930|\n",
      "+------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classificationmodel = lr\n",
    "spk_trainsize = 0.75\n",
    "spk_testsize = 1 - spk_trainsize\n",
    "df_spk_train, df_spk_test = df_spk_use.randomSplit([spk_trainsize, spk_testsize], seed=12345)\n",
    "spk_classification(classificationmodel,df_spk_train,df_spk_test)\n",
    "spk_crossvalidation(classificationmodel,pipeline,evaluation,df_spk_train,df_spk_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4.4'></a>\n",
    "____\n",
    "#### 4.4 Model 2 - Random Forest\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Random Forest \n",
      "--------------------------------------\n",
      "Description: RandomForestClassificationModel (uid=RandomForestClassifier_b3a4c4dfb3b3) with 10 trees\n",
      "Trainset Size (%): 0.8 \n",
      "Testset Size (%): 0.2\n",
      "Trainset Count: 143084 \n",
      "Testset Count: 35747\n",
      "Trainset Accuracy: 1.0\n",
      "Trainset Error = 0\n",
      "Testset Accuracy: 1.0\n",
      "Test Error = 0\n",
      "\n",
      "Trainset Results: \n",
      "+------+----------+-----+\n",
      "|Injury|prediction|count|\n",
      "+------+----------+-----+\n",
      "|     0|       0.0|98862|\n",
      "|     1|       1.0|44222|\n",
      "+------+----------+-----+\n",
      "\n",
      "\n",
      "Testset Results: \n",
      "+------+----------+-----+\n",
      "|Injury|prediction|count|\n",
      "+------+----------+-----+\n",
      "|     0|       0.0|24555|\n",
      "|     1|       1.0|11192|\n",
      "+------+----------+-----+\n",
      "\n",
      "Cross Validation Results:\n",
      "--------------------------------------\n",
      "+------+----------+-----+\n",
      "|Injury|prediction|count|\n",
      "+------+----------+-----+\n",
      "|     0|       0.0|24555|\n",
      "|     1|       1.0|11192|\n",
      "+------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classificationmodel = rf\n",
    "spk_trainsize = 0.8\n",
    "spk_testsize = 1 - spk_trainsize\n",
    "df_spk_train, df_spk_test = df_spk_use.randomSplit([spk_trainsize, spk_testsize], seed=12345)\n",
    "spk_classification(classificationmodel,df_spk_train,df_spk_test)\n",
    "spk_crossvalidation(classificationmodel,pipeline,evaluation,df_spk_train,df_spk_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4.5'></a>\n",
    "____\n",
    "#### 4.5 Model 3 - Gradient Boosted Tree\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Gradient Boosted Tree \n",
      "--------------------------------------\n",
      "Description: GBTClassificationModel (uid=GBTClassifier_e24b34312d3f) with 10 trees\n",
      "Trainset Size (%): 0.85 \n",
      "Testset Size (%): 0.15\n",
      "Trainset Count: 152104 \n",
      "Testset Count: 26727\n",
      "Trainset Accuracy: 1.0\n",
      "Trainset Error = 0\n",
      "Testset Accuracy: 1.0\n",
      "Test Error = 0\n",
      "\n",
      "Trainset Results: \n",
      "+------+----------+------+\n",
      "|Injury|prediction| count|\n",
      "+------+----------+------+\n",
      "|     0|       0.0|104979|\n",
      "|     1|       1.0| 47125|\n",
      "+------+----------+------+\n",
      "\n",
      "\n",
      "Testset Results: \n",
      "+------+----------+-----+\n",
      "|Injury|prediction|count|\n",
      "+------+----------+-----+\n",
      "|     0|       0.0|18438|\n",
      "|     1|       1.0| 8289|\n",
      "+------+----------+-----+\n",
      "\n",
      "Cross Validation Results:\n",
      "--------------------------------------\n",
      "+------+----------+-----+\n",
      "|Injury|prediction|count|\n",
      "+------+----------+-----+\n",
      "|     0|       0.0|18438|\n",
      "|     1|       1.0| 8289|\n",
      "+------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classificationmodel = gbt\n",
    "spk_trainsize = 0.85\n",
    "spk_testsize = 1 - spk_trainsize\n",
    "df_spk_train, df_spk_test = df_spk_use.randomSplit([spk_trainsize, spk_testsize], seed=12345)\n",
    "spk_classification(classificationmodel,df_spk_train,df_spk_test)\n",
    "spk_crossvalidation(classificationmodel,pipeline,evaluation,df_spk_train,df_spk_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4.6'></a>\n",
    "____\n",
    "#### 4.6 Model 4 - Linear Support Vector\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Linear Support Vector Machine \n",
      "--------------------------------------\n",
      "Description: LinearSVC_b996fb5ef15a\n",
      "Trainset Size (%): 0.82 \n",
      "Testset Size (%): 0.18\n",
      "Trainset Count: 146756 \n",
      "Testset Count: 32075\n",
      "Trainset Accuracy: 0.993\n",
      "Trainset Error = 0.007\n",
      "Testset Accuracy: 0.993\n",
      "Test Error = 0.007\n",
      "\n",
      "Trainset Results: \n",
      "+------+----------+------+\n",
      "|Injury|prediction| count|\n",
      "+------+----------+------+\n",
      "|     0|       1.0|   978|\n",
      "|     0|       0.0|100348|\n",
      "|     1|       1.0| 45430|\n",
      "+------+----------+------+\n",
      "\n",
      "\n",
      "Testset Results: \n",
      "+------+----------+-----+\n",
      "|Injury|prediction|count|\n",
      "+------+----------+-----+\n",
      "|     0|       1.0|  217|\n",
      "|     0|       0.0|21874|\n",
      "|     1|       1.0| 9984|\n",
      "+------+----------+-----+\n",
      "\n",
      "Cross Validation Results:\n",
      "--------------------------------------\n",
      "+------+----------+-----+\n",
      "|Injury|prediction|count|\n",
      "+------+----------+-----+\n",
      "|     0|       1.0|  217|\n",
      "|     0|       0.0|21874|\n",
      "|     1|       1.0| 9984|\n",
      "+------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classificationmodel = lsvc\n",
    "spk_trainsize = 0.82\n",
    "spk_testsize = 1 - spk_trainsize\n",
    "df_spk_train, df_spk_test = df_spk_use.randomSplit([spk_trainsize, spk_testsize], seed=12345)\n",
    "spk_classification(classificationmodel,df_spk_train,df_spk_test)\n",
    "spk_crossvalidation(classificationmodel,pipeline,evaluation,df_spk_train,df_spk_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5'></a>\n",
    "____\n",
    "### 5. Conclusion\n",
    "\n",
    "The accuracy score across all four models is very high which is a very good outcome. \n",
    "\n",
    "______\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<strong> <center> Thank You! :)</s>\n",
    " _____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
